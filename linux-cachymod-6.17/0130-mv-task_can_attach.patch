
sched: cgroup: Move task_can_attach() to cpuset.c
https://lore.kernel.org/all/20251003121421.0cf4372d@gandalf.local.home/

At our monthly stable meeting, we were talking about documenting non
static functions and randomly picked a function to look at. That was
task_can_attach(). It was then noticed that it's only used by
cgroup/cpuset.c and nothing else. It's a simple function that doesn't
reference anything unique to sched/core.c, hence there's no reason that
function should be there.

Move it to cgroup/cpuset.c as that's the only place it is used. Also make
it a static inline as it is so small.

Signed-off-by: Steven Rostedt (Google) <rostedt@xxxxxxxxxx>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e4ce0a76831e..4ee4fa973eda 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1849,7 +1849,6 @@ current_restore_flags(unsigned long orig_flags, unsigned long flags)
 }
 
 extern int cpuset_cpumask_can_shrink(const struct cpumask *cur, const struct cpumask *trial);
-extern int task_can_attach(struct task_struct *p);
 extern int dl_bw_alloc(int cpu, u64 dl_bw);
 extern void dl_bw_free(int cpu, u64 dl_bw);
 
diff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c
index 27adb04df675..21fe872803e8 100644
--- a/kernel/cgroup/cpuset.c
+++ b/kernel/cgroup/cpuset.c
@@ -3009,6 +3009,25 @@ static void reset_migrate_dl_data(struct cpuset *cs)
 	cs->sum_migrate_dl_bw = 0;
 }
 
+static inline int task_can_attach(struct task_struct *p)
+{
+	int ret = 0;
+
+	/*
+	 * Kthreads which disallow setaffinity shouldn't be moved
+	 * to a new cpuset; we don't want to change their CPU
+	 * affinity and isolating such threads by their set of
+	 * allowed nodes is unnecessary.  Thus, cpusets are not
+	 * applicable for such threads.  This prevents checking for
+	 * success of set_cpus_allowed_ptr() on all attached tasks
+	 * before cpus_mask may be changed.
+	 */
+	if (p->flags & PF_NO_SETAFFINITY)
+		ret = -EINVAL;
+
+	return ret;
+}
+
 /* Called by cgroups to determine if a cpuset is usable; cpuset_mutex held */
 static int cpuset_can_attach(struct cgroup_taskset *tset)
 {
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index ccba6fc3c3fe..a195c4b25475 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -8070,25 +8070,6 @@ int cpuset_cpumask_can_shrink(const struct cpumask *cur,
 	return ret;
 }
 
-int task_can_attach(struct task_struct *p)
-{
-	int ret = 0;
-
-	/*
-	 * Kthreads which disallow setaffinity shouldn't be moved
-	 * to a new cpuset; we don't want to change their CPU
-	 * affinity and isolating such threads by their set of
-	 * allowed nodes is unnecessary.  Thus, cpusets are not
-	 * applicable for such threads.  This prevents checking for
-	 * success of set_cpus_allowed_ptr() on all attached tasks
-	 * before cpus_mask may be changed.
-	 */
-	if (p->flags & PF_NO_SETAFFINITY)
-		ret = -EINVAL;
-
-	return ret;
-}
-
 bool sched_smp_initialized __read_mostly;
 
 #ifdef CONFIG_NUMA_BALANCING
-- 
2.50.1


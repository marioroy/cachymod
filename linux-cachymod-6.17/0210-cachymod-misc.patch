Switch to gnu17, a GCC bug-fix version of the C11 standard.

What is C17 and what changes have been made to the language?
https://stackoverflow.com/questions/47529854/

Signed-off-by: Mario Roy <...>

diff -uarp a/arch/x86/Makefile b/arch/x86/Makefile
--- a/arch/x86/Makefile
+++ b/arch/x86/Makefile
@@ -47,7 +47,7 @@ endif
 
 # How to compile the 16-bit code.  Note we always compile for -march=i386;
 # that way we can complain to the user if the CPU is insufficient.
-REALMODE_CFLAGS	:= -std=gnu11 -m16 -g -Os -DDISABLE_BRANCH_PROFILING -D__DISABLE_EXPORTS \
+REALMODE_CFLAGS	:= -std=gnu17 -m16 -O2 -DDISABLE_BRANCH_PROFILING -D__DISABLE_EXPORTS \
 		   -Wall -Wstrict-prototypes -march=i386 -mregparm=3 \
 		   -fno-strict-aliasing -fomit-frame-pointer -fno-pic \
 		   -mno-mmx -mno-sse $(call cc-option,-fcf-protection=none)
diff -uarp a/arch/x86/boot/compressed/Makefile b/arch/x86/boot/compressed/Makefile
--- a/arch/x86/boot/compressed/Makefile
+++ b/arch/x86/boot/compressed/Makefile
@@ -25,7 +25,7 @@ targets := vmlinux vmlinux.bin vmlinux.bin.gz vmlinux.bin.bz2 vmlinux.bin.lzma \
 # avoid errors with '-march=i386', and future flags may depend on the target to
 # be valid.
 KBUILD_CFLAGS := -m$(BITS) -O2 $(CLANG_FLAGS)
-KBUILD_CFLAGS += -std=gnu11
+KBUILD_CFLAGS += -std=gnu17
 KBUILD_CFLAGS += -fno-strict-aliasing -fPIE
 KBUILD_CFLAGS += -Wundef
 KBUILD_CFLAGS += -DDISABLE_BRANCH_PROFILING
diff -uarp a/drivers/firmware/efi/libstub/Makefile b/drivers/firmware/efi/libstub/Makefile
--- a/drivers/firmware/efi/libstub/Makefile
+++ b/drivers/firmware/efi/libstub/Makefile
@@ -11,7 +11,7 @@ cflags-y			:= $(KBUILD_CFLAGS)
 
 cflags-$(CONFIG_X86_32)		:= -march=i386
 cflags-$(CONFIG_X86_64)		:= -mcmodel=small
-cflags-$(CONFIG_X86)		+= -m$(BITS) -D__KERNEL__ -std=gnu11 \
+cflags-$(CONFIG_X86)		+= -m$(BITS) -D__KERNEL__ -std=gnu17 \
 				   -fPIC -fno-strict-aliasing -mno-red-zone \
 				   -mno-mmx -mno-sse -fshort-wchar \
 				   -Wno-pointer-sign \
diff -uarp a/Makefile b/Makefile
--- a/Makefile
+++ b/Makefile
@@ -464,7 +464,7 @@ HOSTRUSTC = rustc
 export KERNELDOC
 
 KBUILD_USERHOSTCFLAGS := -Wall -Wmissing-prototypes -Wstrict-prototypes \
-			 -O2 -fomit-frame-pointer -std=gnu11
+			 -O2 -fomit-frame-pointer -std=gnu17
 KBUILD_USERCFLAGS  := $(KBUILD_USERHOSTCFLAGS) $(USERCFLAGS)
 KBUILD_USERLDFLAGS := $(USERLDFLAGS)
 
@@ -584,7 +584,7 @@ LINUXINCLUDE    := \
 KBUILD_AFLAGS   := -D__ASSEMBLY__ -fno-PIE
 
 KBUILD_CFLAGS :=
-KBUILD_CFLAGS += -std=gnu11
+KBUILD_CFLAGS += -std=gnu17
 KBUILD_CFLAGS += -fshort-wchar
 KBUILD_CFLAGS += -funsigned-char
 KBUILD_CFLAGS += -fno-common
-- 
2.30.1

Speed up compression

diff -uarp a/scripts/Makefile.lib b/scripts/Makefile.lib
--- a/scripts/Makefile.lib
+++ b/scripts/Makefile.lib
@@ -452,13 +452,13 @@ quiet_cmd_xzmisc = XZMISC  $@
 # be used because it would require zstd to allocate a 128 MB buffer.
 
 quiet_cmd_zstd = ZSTD    $@
-      cmd_zstd = cat $(real-prereqs) | $(ZSTD) -19 > $@
+      cmd_zstd = cat $(real-prereqs) | $(ZSTD) -6 > $@
 
 quiet_cmd_zstd22 = ZSTD22  $@
-      cmd_zstd22 = cat $(real-prereqs) | $(ZSTD) -22 --ultra > $@
+      cmd_zstd22 = cat $(real-prereqs) | $(ZSTD) -6 --ultra > $@
 
 quiet_cmd_zstd22_with_size = ZSTD22  $@
-      cmd_zstd22_with_size = { cat $(real-prereqs) | $(ZSTD) -22 --ultra; $(size_append); } > $@
+      cmd_zstd22_with_size = { cat $(real-prereqs) | $(ZSTD) -6 --ultra; $(size_append); } > $@
 
 # ASM offsets
 # ---------------------------------------------------------------------------
-- 
2.30.1

Curated patches from XanMod Linux
https://gitlab.com/xanmod/linux-patches

# 0008-XANMOD-block-mq-deadline-Increase-write-priority-to-.patch

Subject: [PATCH 08/20] XANMOD: block/mq-deadline: Increase write priority to
 improve responsiveness

Signed-off-by: Alexandre Frade <kernel@xxxxxxxxxx>
---
 block/mq-deadline.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/block/mq-deadline.c b/block/mq-deadline.c
index b9b7cdf1d3c9..eb2007d3d5d9 100644
--- a/block/mq-deadline.c
+++ b/block/mq-deadline.c
@@ -28,7 +28,7 @@
  * See Documentation/block/deadline-iosched.rst
  */
 static const int read_expire = HZ / 2;  /* max time before a read is submitted. */
-static const int write_expire = 5 * HZ; /* ditto for writes, these limits are SOFT! */
+static const int write_expire = HZ;     /* ditto for writes, these limits are SOFT! */
 /*
  * Time after which to dispatch lower priority requests even if higher
  * priority requests are pending.
-- 
2.47.2

# 0009-XANMOD-block-mq-deadline-Disable-front_merges-by-def.patch

Subject: [PATCH 09/20] XANMOD: block/mq-deadline: Disable front_merges by
 default

Signed-off-by: Alexandre Frade <kernel@xxxxxxxxxx>
---
 block/mq-deadline.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/block/mq-deadline.c b/block/mq-deadline.c
index eb2007d3d5d9..4463f6736cf6 100644
--- a/block/mq-deadline.c
+++ b/block/mq-deadline.c
@@ -591,7 +591,7 @@ static int dd_init_sched(struct request_queue *q, struct elevator_queue *eq)
 	dd->fifo_expire[DD_READ] = read_expire;
 	dd->fifo_expire[DD_WRITE] = write_expire;
 	dd->writes_starved = writes_starved;
-	dd->front_merges = 1;
+	dd->front_merges = 0;
 	dd->last_dir = DD_WRITE;
 	dd->fifo_batch = fifo_batch;
 	dd->prio_aging_expire = prio_aging_expire;
-- 
2.47.2

# 0010-XANMOD-block-Set-rq_affinity-to-force-complete-I-O-r.patch

Subject: [PATCH 10/20] XANMOD: block: Set rq_affinity to force complete I/O
 requests on same CPU

Signed-off-by: Alexandre Frade <kernel@xxxxxxxxxx>
---
 include/linux/blkdev.h | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index fe1797bbec42..96deb7a7a8fc 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -660,7 +660,8 @@ enum {
 	QUEUE_FLAG_MAX
 };
 
-#define QUEUE_FLAG_MQ_DEFAULT	(1UL << QUEUE_FLAG_SAME_COMP)
+#define QUEUE_FLAG_MQ_DEFAULT	((1UL << QUEUE_FLAG_SAME_COMP) |		\
+				 (1UL << QUEUE_FLAG_SAME_FORCE))
 
 void blk_queue_flag_set(unsigned int flag, struct request_queue *q);
 void blk_queue_flag_clear(unsigned int flag, struct request_queue *q);
-- 
2.47.2

# 0011-XANMOD-blk-wbt-Set-wbt_default_latency_nsec-to-2msec.patch

Subject: [PATCH 11/20] XANMOD: blk-wbt: Set wbt_default_latency_nsec() to
 2msec

Signed-off-by: Alexandre Frade <kernel@xxxxxxxxxx>
---
 block/blk-wbt.c | 10 ++--------
 1 file changed, 2 insertions(+), 8 deletions(-)

diff --git a/block/blk-wbt.c b/block/blk-wbt.c
index eb8037bae0bd..fe2d29903ae2 100644
--- a/block/blk-wbt.c
+++ b/block/blk-wbt.c
@@ -731,14 +731,8 @@ EXPORT_SYMBOL_GPL(wbt_enable_default);
 
 u64 wbt_default_latency_nsec(struct request_queue *q)
 {
-	/*
-	 * We default to 2msec for non-rotational storage, and 75msec
-	 * for rotational storage.
-	 */
-	if (blk_queue_nonrot(q))
-		return 2000000ULL;
-	else
-		return 75000000ULL;
+	/* XanMod defaults to 2msec for any type of storage */
+	return 2000000ULL;
 }
 
 static int wbt_data_dir(const struct request *rq)
-- 
2.47.2

# 0013-XANMOD-vfs-Decrease-rate-at-which-vfs-caches-are-rec.patch

Subject: [PATCH 13/20] XANMOD: vfs: Decrease rate at which vfs caches are
 reclaimed

Signed-off-by: Alexandre Frade <kernel@xxxxxxxxxx>
---
 fs/dcache.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/fs/dcache.c b/fs/dcache.c
index 60046ae23d51..1d1b67910b29 100644
--- a/fs/dcache.c
+++ b/fs/dcache.c
@@ -73,7 +73,7 @@
  * If no ancestor relationship:
  * arbitrary, since it's serialized on rename_lock
  */
-static int sysctl_vfs_cache_pressure __read_mostly = 100;
+static int sysctl_vfs_cache_pressure __read_mostly = 50;
 static int sysctl_vfs_cache_pressure_denom __read_mostly = 100;
 
 unsigned long vfs_pressure_ratio(unsigned long val)
-- 
2.47.2

Subject: [PATCH 2/2] ZEN: dm-crypt: Disable workqueues for crypto ops

Queueing in dm-crypt for crypto operations reduces performance on modern
systems.  As discussed in an article from Cloudflare, they discovered
that queuing was introduced because the crypto subsystem used to be
synchronous.  Since it's now asynchronous, we get double queueing when
using the subsystem through dm-crypt.  This is obviously undesirable and
reduces throughput and increases latency.

Fixes: https://github.com/zen-kernel/zen-kernel/issues/282
Signed-off-by: Alexandre Frade <kernel@xxxxxxxxxx>
---
 drivers/md/dm-crypt.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/drivers/md/dm-crypt.c b/drivers/md/dm-crypt.c
index 5ef43231fe77..c6f9815830e1 100644
--- a/drivers/md/dm-crypt.c
+++ b/drivers/md/dm-crypt.c
@@ -3305,6 +3305,9 @@ static int crypt_ctr(struct dm_target *ti, unsigned int argc, char **argv)
 			goto bad;
 	}
 
+	set_bit(DM_CRYPT_NO_READ_WORKQUEUE, &cc->flags);
+	set_bit(DM_CRYPT_NO_WRITE_WORKQUEUE, &cc->flags);
+
 	ret = crypt_ctr_cipher(ti, argv[0], argv[1]);
 	if (ret < 0)
 		goto bad;
-- 
2.47.2

Curated patches from SUNLIGHT Linux
https://github.com/sunlightlinux/linux-sunlight/commits/6.17

SUNLIGHT: x86/tsc: Use rdtsc_ordered() when RDTSCP or LFENCE_RDTSC are supported
https://github.com/sunlightlinux/linux-sunlight/commit/1966f2297775

On AMD processors the TSC has been reported drifting on and off for
various platforms. This has been root caused to becaused by out of order
TSC and HPET counter values. When the SoC supports RDTSCP or LFENCE_RDTSC
use ordered tsc reads instead.

Signed-off-by: Mario Limonciello <mario.limonciello@xxxxxxxxxx>
Signed-off-by: Ionut Nechita <ionut_n2001@xxxxxxxxxx>
---
 arch/x86/include/asm/tsc.h | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -79,6 +79,9 @@ static inline cycles_t get_cycles(void)
 	if (!IS_ENABLED(CONFIG_X86_TSC) &&
 	    !cpu_feature_enabled(X86_FEATURE_TSC))
 		return 0;
+	if (cpu_feature_enabled(X86_FEATURE_LFENCE_RDTSC) ||
+	    cpu_feature_enabled(X86_FEATURE_RDTSCP))
+		return rdtsc_ordered();
 	return rdtsc();
 }
 #define get_cycles get_cycles
-- 
2.43.0

SUNLIGHT: Change default value for wq_cpu_intensive_thresh_us
https://github.com/sunlightlinux/linux-sunlight/commit/3e7ec6c34073

Description:
 - 10ms -> 30ms (edit: change to 20ms)
 - By using new processors it helps to have
   a higher threshold for thresh

Signed-off-by: Ionut Nechita <ionut_n2001@xxxxxxxxxx>
Signed-off-by: Mario Roy <...>
---
 kernel/workqueue.c | 8 ++++----
 1 file changed, 4 insertions(+), 4 deletions(-)

diff --git a/kernel/workqueue.c b/kernel/workqueue.c
--- a/kernel/workqueue.c
+++ b/kernel/workqueue.c
@@ -7864,20 +7864,20 @@ static void __init wq_cpu_intensive_thresh_init(void)
 		return;
 
 	/*
-	 * The default of 10ms is derived from the fact that most modern (as of
-	 * 2023) processors can do a lot in 10ms and that it's just below what
+	 * The default of 20ms is derived from the fact that most modern (as of
+	 * 2023) processors can do a lot in 20ms and that it's just below what
 	 * most consider human-perceivable. However, the kernel also runs on a
 	 * lot slower CPUs including microcontrollers where the threshold is way
 	 * too low.
 	 *
-	 * Let's scale up the threshold upto 1 second if BogoMips is below 4000.
+	 * Let's scale up the threshold upto 2 second if BogoMips is below 4000.
 	 * This is by no means accurate but it doesn't have to be. The mechanism
 	 * is still useful even when the threshold is fully scaled up. Also, as
 	 * the reports would usually be applicable to everyone, some machines
 	 * operating on longer thresholds won't significantly diminish their
 	 * usefulness.
 	 */
-	thresh = 10 * USEC_PER_MSEC;
+	thresh = 20 * USEC_PER_MSEC;
 
 	/* see init/calibrate.c for lpj -> BogoMIPS calculation */
 	bogo = max_t(unsigned long, loops_per_jiffy / 500000 * HZ, 1);
-- 
2.43.0

SUNLIGHT: sched/core: Further optimize sched_move_task by avoiding
  lock acquisition when autogroup is enabled
https://github.com/sunlightlinux/linux-sunlight/commit/05a5c9633543

Description:
 - Some sched_move_task calls are useless because task_struct->sched_task_group
   might not change (remains equal to task_group of cpu_cgroup) when the system
   has autogroup enabled. This patch optimizes the process by:

   1. Splitting sched_change_group() into two functions: one to get the task_group
   and another to perform the actual group change
   2. Adding a new sched_needs_group_change() function to check if a change is needed
   3. Most importantly, performing this check *before* acquiring the runqueue lock
   to completely avoid lock acquisition when no change is needed

   This approach provides a significant performance improvement over the original
   patch, reducing the time spent in sched_move_task by 74-96% (compared to the
   57.4% improvement in the original patch) in test scenarios involving frequent
   task creation and exit.

Based on the original idea from:
Link: https://lkml.kernel.org/r/20230321064459.39421-1-wuchi.zero@gmail.com
Signed-off-by: Ionut Nechita <ionut_n2001@xxxxxxxxxx>
---
 kernel/sched/core.c | 27 ++++++++++++++++++++++++---
 1 file changed, 24 insertions(+), 3 deletions(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 2e61c9707bea..fa7b0d1e8be6 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -9189,7 +9189,7 @@ void sched_release_group(struct task_group *tg)
 	spin_unlock_irqrestore(&task_group_lock, flags);
 }
 
-static void sched_change_group(struct task_struct *tsk)
+static struct task_group *sched_get_task_group(struct task_struct *tsk)
 {
 	struct task_group *tg;
 
@@ -9201,8 +9201,13 @@ static void sched_change_group(struct task_struct *tsk)
 	tg = container_of(task_css_check(tsk, cpu_cgrp_id, true),
 			  struct task_group, css);
 	tg = autogroup_task_group(tsk, tg);
-	tsk->sched_task_group = tg;
 
+	return tg;
+}
+
+static void sched_change_group(struct task_struct *tsk, struct task_group *group)
+{
+	tsk->sched_task_group = group;
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	if (tsk->sched_class->task_change_group)
 		tsk->sched_class->task_change_group(tsk);
@@ -9211,6 +9216,18 @@ static void sched_change_group(struct task_struct *tsk)
 		set_task_rq(tsk, task_cpu(tsk));
 }
 
+static struct task_group *sched_needs_group_change(struct task_struct *tsk)
+{
+	struct task_group *new_group;
+
+	new_group = sched_get_task_group(tsk);
+
+	if (likely(new_group == tsk->sched_task_group))
+		return NULL;
+
+	return new_group;
+}
+
 /*
  * Change task's runqueue when it moves between groups.
  *
@@ -9220,10 +9237,14 @@ static void sched_change_group(struct task_struct *tsk)
  */
 void sched_move_task(struct task_struct *tsk, bool for_autogroup)
 {
+	struct task_group *new_group;
 	int queued, running, queue_flags =
 		DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
 	struct rq *rq;
 
+	if (!(new_group = sched_needs_group_change(tsk)))
+		return;
+
 	CLASS(task_rq_lock, rq_guard)(tsk);
 	rq = rq_guard.rq;
 
@@ -9237,7 +9258,7 @@ void sched_move_task(struct task_struct *tsk, bool for_autogroup)
 	if (running)
 		put_prev_task(rq, tsk);
 
-	sched_change_group(tsk);
+	sched_change_group(tsk, new_group);
 	if (!for_autogroup)
 		scx_cgroup_move_task(tsk);
 
-- 
2.43.0

SUNLIGHT: sched: Add comprehensive micro-optimizations for scheduler performance
https://github.com/sunlightlinux/linux-sunlight/commit/c755f440829c

Description:
 - Implement cache-line alignment and prefetch optimizations for critical
   scheduler hot paths to reduce cache misses and improve performance:

   - Add cache-line alignment for frequently accessed structures (rq, cfs_rq, rt_rq)
   - Implement prefetch instructions in sched_move_task() for task and runqueue structures
   - Optimize task_on_rq_queued() and task_current_donor() with likely() and prefetch
   - Add targeted prefetching for dequeue/enqueue operations

   These micro-optimizations target the most performance-critical scheduler paths
   to minimize cache contention and improve branch prediction accuracy.

Signed-off-by: Ionut Nechita <ionut_n2001@xxxxxxxxxx>
---
 kernel/sched/core.c  | 34 ++++++++++++++++++++++++++++++----
 kernel/sched/sched.h | 32 ++++++++++++++++++--------------
 2 files changed, 48 insertions(+), 18 deletions(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index fa7b0d1e8be6..4489a5673efa 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -9218,13 +9218,22 @@ static void sched_change_group(struct task_struct *tsk, struct task_group *group
 
 static struct task_group *sched_needs_group_change(struct task_struct *tsk)
 {
-	struct task_group *new_group;
+	struct task_group *new_group, *current_group;
+
+	/* Cache current group to reduce pointer dereferences */
+	current_group = tsk->sched_task_group;
+
+	/* Prefetch task group structures for better cache performance */
+	if (likely(current_group))
+		prefetch(current_group);
 
 	new_group = sched_get_task_group(tsk);
 
-	if (likely(new_group == tsk->sched_task_group))
+	if (likely(new_group == current_group))
 		return NULL;
 
+	/* Prefetch new group structures */
+	prefetch(new_group);
 	return new_group;
 }
 
@@ -9245,16 +9254,29 @@ void sched_move_task(struct task_struct *tsk, bool for_autogroup)
 	if (!(new_group = sched_needs_group_change(tsk)))
 		return;
 
+	/* Prefetch task structure for upcoming operations */
+	prefetch(&tsk->se);
+	prefetch(&tsk->se.cfs_rq);
+
 	CLASS(task_rq_lock, rq_guard)(tsk);
 	rq = rq_guard.rq;
 
+	/* Prefetch runqueue structures for better cache performance */
+	prefetch(&rq->cfs);
+	prefetch(&rq->rt);
+	prefetch(&rq->dl);
+
 	update_rq_clock(rq);
 
 	running = task_current_donor(rq, tsk);
 	queued = task_on_rq_queued(tsk);
 
-	if (queued)
+	if (queued) {
+		/* Prefetch scheduler entity before dequeue */
+		prefetch(&tsk->se.on_rq);
+		prefetch(&tsk->se.load);
 		dequeue_task(rq, tsk, queue_flags);
+	}
 	if (running)
 		put_prev_task(rq, tsk);
 
@@ -9262,8 +9284,12 @@ void sched_move_task(struct task_struct *tsk, bool for_autogroup)
 	if (!for_autogroup)
 		scx_cgroup_move_task(tsk);
 
-	if (queued)
+	if (queued) {
+		/* Prefetch scheduler entity before enqueue */
+		prefetch(&tsk->se.avg);
+		prefetch(&tsk->se.vruntime);
 		enqueue_task(rq, tsk, queue_flags);
+	}
 	if (running) {
 		set_next_task(rq, tsk);
 		/*
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c2bebc6f1035..a702b076497c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -673,13 +673,13 @@ struct balance_callback {
 
 /* CFS-related fields in a runqueue */
 struct cfs_rq {
-	struct load_weight	load;
+	struct load_weight	load ____cacheline_aligned;
 	unsigned int		nr_queued;
 	unsigned int		h_nr_queued;       /* SCHED_{NORMAL,BATCH,IDLE} */
 	unsigned int		h_nr_runnable;     /* SCHED_{NORMAL,BATCH,IDLE} */
 	unsigned int		h_nr_idle; /* SCHED_IDLE */
 
-	s64			avg_vruntime;
+	s64			avg_vruntime ____cacheline_aligned;
 	u64			avg_load;
 
 	u64			min_vruntime;
@@ -821,8 +821,8 @@ static inline int rt_bandwidth_enabled(void)
 
 /* Real-Time classes' related field in a runqueue: */
 struct rt_rq {
-	struct rt_prio_array	active;
-	unsigned int		rt_nr_running;
+	struct rt_prio_array	active ____cacheline_aligned;
+	unsigned int		rt_nr_running ____cacheline_aligned;
 	unsigned int		rr_nr_running;
 	struct {
 		int		curr; /* highest queued rt task prio */
@@ -1115,9 +1115,10 @@ DECLARE_STATIC_KEY_FALSE(sched_uclamp_used);
  */
 struct rq {
 	/* runqueue lock: */
-	raw_spinlock_t		__lock;
+	raw_spinlock_t		__lock ____cacheline_aligned;
 
-	unsigned int		nr_running;
+	/* Hot fields - frequently accessed together */
+	unsigned int		nr_running ____cacheline_aligned;
 #ifdef CONFIG_NUMA_BALANCING
 	unsigned int		nr_numa_running;
 	unsigned int		nr_preferred_running;
@@ -1141,9 +1142,9 @@ struct rq {
 #define UCLAMP_FLAG_IDLE 0x01
 #endif
 
-	struct cfs_rq		cfs;
-	struct rt_rq		rt;
-	struct dl_rq		dl;
+	struct cfs_rq		cfs ____cacheline_aligned;
+	struct rt_rq		rt ____cacheline_aligned;
+	struct dl_rq		dl ____cacheline_aligned;
 #ifdef CONFIG_SCHED_CLASS_EXT
 	struct scx_rq		scx;
 #endif
@@ -1171,7 +1172,7 @@ struct rq {
 	union {
 		struct task_struct __rcu *donor; /* Scheduler context */
 		struct task_struct __rcu *curr;  /* Execution context */
-	};
+	} ____cacheline_aligned;
 #endif
 	struct sched_dl_entity	*dl_server;
 	struct task_struct	*idle;
@@ -1179,10 +1180,10 @@ struct rq {
 	unsigned long		next_balance;
 	struct mm_struct	*prev_mm;
 
-	unsigned int		clock_update_flags;
+	unsigned int		clock_update_flags ____cacheline_aligned;
 	u64			clock;
 	/* Ensure that all clocks are in the same cache line */
-	u64			clock_task ____cacheline_aligned;
+	u64			clock_task;
 	u64			clock_pelt;
 	unsigned long		lost_idle_time;
 	u64			clock_pelt_idle;
@@ -2281,7 +2282,8 @@ static inline int task_current(struct rq *rq, struct task_struct *p)
  */
 static inline int task_current_donor(struct rq *rq, struct task_struct *p)
 {
-	return rq->donor == p;
+	/* Likely branch optimization for common case */
+	return likely(rq->donor == p);
 }
 
 static inline bool task_is_blocked(struct task_struct *p)
@@ -2299,7 +2301,9 @@ static inline int task_on_cpu(struct rq *rq, struct task_struct *p)
 
 static inline int task_on_rq_queued(struct task_struct *p)
 {
-	return READ_ONCE(p->on_rq) == TASK_ON_RQ_QUEUED;
+	/* Prefetch task structure for better cache performance */
+	prefetch(&p->se);
+	return likely(READ_ONCE(p->on_rq) == TASK_ON_RQ_QUEUED);
 }
 
 static inline int task_on_rq_migrating(struct task_struct *p)
-- 
2.43.0

SUNLIGHT: x86/irq: Add comprehensive ultra-low latency optimizations
https://github.com/sunlightlinux/linux-sunlight/commit/d0777e64b8f6

Description:
 - Implement extensive IRQ handling micro-optimizations for ultra-low latency
   systems, targeting critical interrupt processing paths:

   Cache Performance Optimizations:
    - Add aggressive prefetch for IRQ descriptors, actions, and handler functions
    - Implement cache-line alignment for irq_err_count and irq_stack_backing_store
    - Add speculative prefetch for adjacent vector entries in ULL configurations

   Dynamic MSI Coalescing Control:
    - Add runtime configurable MSI coalescing via irq_coalesce= kernel parameter
    - Support irq_coalesce=minimal and irq_coalesce=disabled for extreme ULL
    - Reduce coalescing loops from 3 to 2 for PREEMPT, HZ_1000, HZ_800, and
      NO_HZ_FULL configs

   Branch Prediction & Hot Path Optimizations:
    - Add likely/unlikely annotations for interrupt processing paths
    - Implement inc_irq_stat_fast() with prefetch for ULL statistics updates
    - Optimize ack_bad_irq() with unlikely() for error path minimization

   These optimizations specifically target CONFIG_PREEMPT, CONFIG_HZ_1000,
   CONFIG_HZ_800, and CONFIG_NO_HZ_FULL systems to minimize interrupt latency
   without affecting throughput-oriented configurations.

Signed-off-by: Ionut Nechita <ionut_n2001@xxxxxxxxxx>
---
 arch/x86/kernel/irq.c    | 116 +++++++++++++++++++++++++++++++++++----
 arch/x86/kernel/irq_64.c |   2 +-
 2 files changed, 106 insertions(+), 12 deletions(-)

diff --git a/arch/x86/kernel/irq.c b/arch/x86/kernel/irq.c
index 10721a125226..d437790233c3 100644
--- a/arch/x86/kernel/irq.c
+++ b/arch/x86/kernel/irq.c
@@ -12,6 +12,7 @@
 #include <linux/delay.h>
 #include <linux/export.h>
 #include <linux/irq.h>
+#include <linux/prefetch.h>
 
 #include <asm/irq_stack.h>
 #include <asm/apic.h>
@@ -38,7 +39,7 @@ EXPORT_PER_CPU_SYMBOL(__softirq_pending);
 
 DEFINE_PER_CPU_CACHE_HOT(struct irq_stack *, hardirq_stack_ptr);
 
-atomic_t irq_err_count;
+atomic_t irq_err_count ____cacheline_aligned;
 
 /*
  * 'what should we do if we get a hw irq event on an illegal vector'.
@@ -46,7 +47,8 @@ atomic_t irq_err_count;
  */
 void ack_bad_irq(unsigned int irq)
 {
-	if (printk_ratelimit())
+	/* For ULL, minimize printk overhead in interrupt context */
+	if (unlikely(printk_ratelimit()))
 		pr_err("unexpected IRQ trap at vector %02x\n", irq);
 
 	/*
@@ -62,6 +64,17 @@ void ack_bad_irq(unsigned int irq)
 }
 
 #define irq_stats(x)		(&per_cpu(irq_stat, x))
+
+/* ULL optimization: inline hot IRQ stat updates */
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_HZ_1000) || defined(CONFIG_HZ_800) || defined(CONFIG_NO_HZ_FULL)
+#define inc_irq_stat_fast(member) \
+	do { \
+		prefetch(&this_cpu_ptr(&irq_stat)->member); \
+		this_cpu_inc(irq_stat.member); \
+	} while (0)
+#else
+#define inc_irq_stat_fast(member) inc_irq_stat(member)
+#endif
 /*
  * /proc/interrupts printing for arch specific interrupts
  */
@@ -250,6 +263,12 @@ u64 arch_irq_stat(void)
 static __always_inline void handle_irq(struct irq_desc *desc,
 				       struct pt_regs *regs)
 {
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_HZ_1000) || defined(CONFIG_HZ_800) || defined(CONFIG_NO_HZ_FULL)
+	/* ULL: Prefetch handler function for better cache locality */
+	if (likely(desc->action))
+		prefetch(desc->action->handler);
+#endif
+
 	if (IS_ENABLED(CONFIG_X86_64))
 		generic_handle_irq_desc(desc);
 	else
@@ -260,9 +279,11 @@ static struct irq_desc *reevaluate_vector(int vector)
 {
 	struct irq_desc *desc = __this_cpu_read(vector_irq[vector]);
 
-	if (!IS_ERR_OR_NULL(desc))
+	/* Fast path: valid descriptor found */
+	if (likely(!IS_ERR_OR_NULL(desc)))
 		return desc;
 
+	/* Slow path: handle error cases */
 	if (desc == VECTOR_UNUSED)
 		pr_emerg_ratelimited("No irq handler for %d.%u\n", smp_processor_id(), vector);
 	else
@@ -274,7 +295,16 @@ static __always_inline bool call_irq_handler(int vector, struct pt_regs *regs)
 {
 	struct irq_desc *desc = __this_cpu_read(vector_irq[vector]);
 
+	/* Aggressive prefetch for ULL interrupt processing */
 	if (likely(!IS_ERR_OR_NULL(desc))) {
+		prefetch(desc);
+		prefetch(&desc->irq_data);
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_HZ_1000) || defined(CONFIG_HZ_800) || defined(CONFIG_NO_HZ_FULL)
+		/* For ULL, prefetch critical fields that will be accessed */
+		prefetch(&desc->action);
+		prefetch(&desc->irq_data.chip);
+		prefetch(&desc->lock);
+#endif
 		handle_irq(desc, regs);
 		return true;
 	}
@@ -322,6 +352,19 @@ DEFINE_IDTENTRY_IRQ(common_interrupt)
 	/* entry code tells RCU that we're not quiescent.  Check it. */
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "IRQ failed to wake up RCU");
 
+	/* Aggressive prefetch for ULL: prefetch vector_irq and likely next vectors */
+	{
+		struct irq_desc **vector_ptr = this_cpu_ptr(vector_irq);
+		prefetch(&vector_ptr[vector]);
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_HZ_1000) || defined(CONFIG_HZ_800) || defined(CONFIG_NO_HZ_FULL)
+		/* For ULL systems, speculatively prefetch adjacent vector entries */
+		if (likely(vector < NR_VECTORS - 1))
+			prefetch(&vector_ptr[vector + 1]);
+		/* Prefetch IRQ statistics for likely inc_irq_stat() calls */
+		prefetch(this_cpu_ptr(&irq_stat));
+#endif
+	}
+
 	if (unlikely(!call_irq_handler(vector, regs)))
 		apic_eoi();
 
@@ -340,7 +383,7 @@ DEFINE_IDTENTRY_SYSVEC(sysvec_x86_platform_ipi)
 
 	apic_eoi();
 	trace_x86_platform_ipi_entry(X86_PLATFORM_IPI_VECTOR);
-	inc_irq_stat(x86_platform_ipis);
+	inc_irq_stat_fast(x86_platform_ipis);
 	if (x86_platform_ipi_callback)
 		x86_platform_ipi_callback();
 	trace_x86_platform_ipi_exit(X86_PLATFORM_IPI_VECTOR);
@@ -369,7 +412,7 @@ EXPORT_SYMBOL_GPL(kvm_set_posted_intr_wakeup_handler);
 DEFINE_IDTENTRY_SYSVEC_SIMPLE(sysvec_kvm_posted_intr_ipi)
 {
 	apic_eoi();
-	inc_irq_stat(kvm_posted_intr_ipis);
+	inc_irq_stat_fast(kvm_posted_intr_ipis);
 }
 
 /*
@@ -418,9 +461,15 @@ static __always_inline bool handle_pending_pir(unsigned long *pir, struct pt_reg
 	unsigned long pir_copy[NR_PIR_WORDS];
 	int vec = FIRST_EXTERNAL_VECTOR;
 
+	/* Prefetch PIR data for better cache performance */
+	prefetch(pir);
+
 	if (!pi_harvest_pir(pir, pir_copy))
 		return false;
 
+	/* Prefetch pir_copy for the upcoming loop */
+	prefetch(pir_copy);
+
 	for_each_set_bit_from(vec, pir_copy, FIRST_SYSTEM_VECTOR)
 		call_irq_handler(vec, regs);
 
@@ -429,9 +478,47 @@ static __always_inline bool handle_pending_pir(unsigned long *pir, struct pt_reg
 
 /*
  * Performance data shows that 3 is good enough to harvest 90+% of the benefit
- * on high IRQ rate workload.
+ * on high IRQ rate workload. For ultra-low latency systems, reduce to 2 or 1
+ * to minimize interrupt processing time and reduce maximum latency spikes.
  */
+#if defined(CONFIG_PREEMPT_RT) || defined(CONFIG_PREEMPT) || defined(CONFIG_HZ_1000) || defined(CONFIG_HZ_800) || defined(CONFIG_NO_HZ_FULL) || defined(CONFIG_HZ_300) || defined(CONFIG_PREEMPT_VOLUNTARY)
+#define MAX_POSTED_MSI_COALESCING_LOOP 2
+#else
 #define MAX_POSTED_MSI_COALESCING_LOOP 3
+#endif
+
+/*
+ * For extreme ultra-low latency (trading throughput for latency),
+ * reduce coalescing on critical systems via kernel command line:
+ * - irq_coalesce=minimal  : 2 loops (reduced coalescing)
+ * - irq_coalesce=disabled : 1 loop  (minimal processing, extreme ULL)
+ */
+static int irq_coalesce_mode = 0; /* 0=default, 1=minimal, 2=disabled */
+
+static int __init irq_coalesce_setup(char *str)
+{
+	if (!strcmp(str, "minimal"))
+		irq_coalesce_mode = 1;  /* 2 loops */
+	else if (!strcmp(str, "disabled"))
+		irq_coalesce_mode = 2;  /* 1 loop */
+	return 1;
+}
+__setup("irq_coalesce=", irq_coalesce_setup);
+
+/* Dynamic coalescing based on boot parameter */
+static inline int get_msi_coalescing_loop_count(void)
+{
+	if (irq_coalesce_mode == 2)
+		return 1; /* disabled - absolute minimum processing */
+	if (irq_coalesce_mode == 1)
+		return 2; /* minimal - reduced but some coalescing */
+
+#if defined(CONFIG_PREEMPT_RT) || defined(CONFIG_PREEMPT) || defined(CONFIG_HZ_1000) || defined(CONFIG_HZ_800) || defined(CONFIG_NO_HZ_FULL) || defined(CONFIG_HZ_300) || defined(CONFIG_PREEMPT_VOLUNTARY)
+	return 2;
+#else
+	return 3;
+#endif
+}
 
 /*
  * For MSIs that are delivered as posted interrupts, the CPU notifications
@@ -445,16 +532,23 @@ DEFINE_IDTENTRY_SYSVEC(sysvec_posted_msi_notification)
 
 	pid = this_cpu_ptr(&posted_msi_pi_desc);
 
-	inc_irq_stat(posted_msi_notification_count);
+	/* ULL optimization: prefetch PI descriptor fields */
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_HZ_1000) || defined(CONFIG_HZ_800) || defined(CONFIG_NO_HZ_FULL)
+	prefetch(&pid->pir);
+	prefetch(&pid->control);
+#endif
+
+	inc_irq_stat_fast(posted_msi_notification_count);
 	irq_enter();
 
 	/*
 	 * Max coalescing count includes the extra round of handle_pending_pir
-	 * after clearing the outstanding notification bit. Hence, at most
-	 * MAX_POSTED_MSI_COALESCING_LOOP - 1 loops are executed here.
+	 * after clearing the outstanding notification bit. Dynamic coalescing
+	 * allows runtime tuning for ultra-low latency requirements.
 	 */
-	while (++i < MAX_POSTED_MSI_COALESCING_LOOP) {
-		if (!handle_pending_pir(pid->pir, regs))
+	int max_loops = get_msi_coalescing_loop_count();
+	while (likely(++i < max_loops)) {
+		if (unlikely(!handle_pending_pir(pid->pir, regs)))
 			break;
 	}
 
diff --git a/arch/x86/kernel/irq_64.c b/arch/x86/kernel/irq_64.c
index ca78dce39361..ed7859d1a6d8 100644
--- a/arch/x86/kernel/irq_64.c
+++ b/arch/x86/kernel/irq_64.c
@@ -27,7 +27,7 @@
 #include <asm/apic.h>
 
 DEFINE_PER_CPU_CACHE_HOT(bool, hardirq_stack_inuse);
-DEFINE_PER_CPU_PAGE_ALIGNED(struct irq_stack, irq_stack_backing_store) __visible;
+DEFINE_PER_CPU_PAGE_ALIGNED(struct irq_stack, irq_stack_backing_store) __visible ____cacheline_aligned;
 
 #ifdef CONFIG_VMAP_STACK
 /*
-- 
2.43.0

SUNLIGHT: timer: Implement ultra-low latency timer subsystem optimizations
https://github.com/sunlightlinux/linux-sunlight/commit/3a12486d47fa

Description:
 - Implement comprehensive ULL optimizations for the timer subsystem:

   1. Context switch overhead reduction through batch processing:
     - Add prefetch instructions for critical data structures in context_switch()
     - Optimize memory access patterns for next task scheduler entities
     - Add prefetch for user MM structures to reduce page fault latency
     - Convert common paths to likely/unlikely for better branch prediction

   2. Timer hot path optimizations:
     - Add batch processing in run_local_timers() to minimize softirq overhead
     - Prefetch timer base structures before processing
     - Optimize __run_timers() with aggressive prefetching of timer vectors
     - Add prefetch optimizations to __run_timer_base() before lock acquisition
     - Convert timer expiry checks to likely/unlikely for branch optimization

   3. Adaptive tick management for ULL systems:
     - Implement dependency-aware prefetching in tick_nohz_handler()
     - Add aggressive prefetch optimizations in tick_nohz_restart()
     - Optimize can_stop_full_tick() with prefetch for runqueue structures
     - Enhanced cache utilization for NO_HZ_FULL configurations

   These optimizations specifically target:
     - Reduced context switch latency through improved cache utilization
     - Minimized timer interrupt overhead through batching operations
     - Enhanced branch prediction for common timer execution paths
     - Aggressive prefetching to eliminate memory access stalls
     - Optimized tick management for ultra-low latency workloads

   All optimizations maintain full compatibility with existing timer behavior
   while providing significant latency reductions for ULL workloads.

Signed-off-by: Ionut Nechita <ionut_n2001@xxxxxxxxxx>
---
 kernel/sched/core.c      | 16 +++++++++++++--
 kernel/time/tick-sched.c | 20 +++++++++++++++++++
 kernel/time/timer.c      | 43 +++++++++++++++++++++++++++++++---------
 3 files changed, 68 insertions(+), 11 deletions(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index f585830c2c52..144cd8758c6b 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -5301,6 +5301,14 @@ static __always_inline struct rq *
 context_switch(struct rq *rq, struct task_struct *prev,
 	       struct task_struct *next, struct rq_flags *rf)
 {
+	/*
+	 * ULL optimization: prefetch critical data structures early
+	 * to minimize cache misses during context switch hot path
+	 */
+	prefetch(&next->se);
+	prefetch(&next->thread_info);
+	prefetch(&rq->curr);
+
 	prepare_task_switch(rq, prev, next);
 
 	/*
@@ -5320,7 +5328,7 @@ context_switch(struct rq *rq, struct task_struct *prev,
 	 * switch_mm_cid() needs to be updated if the barriers provided
 	 * by context_switch() are modified.
 	 */
-	if (!next->mm) {                                // to kernel
+	if (likely(!next->mm)) {                        // to kernel
 		enter_lazy_tlb(prev->active_mm, next);
 
 		next->active_mm = prev->active_mm;
@@ -5329,6 +5337,10 @@ context_switch(struct rq *rq, struct task_struct *prev,
 		else
 			prev->active_mm = NULL;
 	} else {                                        // to user
+		/* ULL optimization: prefetch user MM structures */
+		prefetch(next->mm);
+		prefetch(&next->mm->mmap_base);
+
 		membarrier_switch_mm(rq, prev->active_mm, next->mm);
 		/*
 		 * sys_membarrier() requires an smp_mb() between setting
@@ -5341,7 +5353,7 @@ context_switch(struct rq *rq, struct task_struct *prev,
 		switch_mm_irqs_off(prev->active_mm, next->mm, next);
 		lru_gen_use_mm(next->mm);
 
-		if (!prev->mm) {                        // from kernel
+		if (unlikely(!prev->mm)) {              // from kernel
 			/* will mmdrop_lazy_tlb() in finish_task_switch(). */
 			rq->prev_mm = prev->active_mm;
 			prev->active_mm = NULL;
diff --git a/kernel/time/tick-sched.c b/kernel/time/tick-sched.c
index fac07b9d55a0..49208b64a3c9 100644
--- a/kernel/time/tick-sched.c
+++ b/kernel/time/tick-sched.c
@@ -27,6 +27,7 @@
 #include <linux/posix-timers.h>
 #include <linux/context_tracking.h>
 #include <linux/mm.h>
+#include <linux/prefetch.h>
 
 #include <asm/irq_regs.h>
 
@@ -287,6 +288,12 @@ static enum hrtimer_restart tick_nohz_handler(struct hrtimer *timer)
 	struct pt_regs *regs = get_irq_regs();
 	ktime_t now = ktime_get();
 
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_HZ_1000) || defined(CONFIG_HZ_800) || defined(CONFIG_NO_HZ_FULL)
+	/* ULL optimization: Prefetch tick_sched structure for upcoming operations */
+	prefetch(&ts->last_jiffies);
+	prefetch(&ts->next_tick);
+#endif
+
 	tick_sched_do_timer(ts, now);
 
 	/*
@@ -359,6 +366,13 @@ static bool can_stop_full_tick(int cpu, struct tick_sched *ts)
 {
 	lockdep_assert_irqs_disabled();
 
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_HZ_1000) || defined(CONFIG_HZ_800) || defined(CONFIG_NO_HZ_FULL)
+	/* ULL optimization: Prefetch dependency structures for faster checks */
+	prefetch(&tick_dep_mask);
+	prefetch(&ts->tick_dep_mask);
+	prefetch(&current->tick_dep_mask);
+#endif
+
 	if (unlikely(!cpu_online(cpu)))
 		return false;
 
@@ -835,6 +849,12 @@ EXPORT_SYMBOL_GPL(get_cpu_iowait_time_us);
 
 static void tick_nohz_restart(struct tick_sched *ts, ktime_t now)
 {
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_HZ_1000) || defined(CONFIG_HZ_800) || defined(CONFIG_NO_HZ_FULL)
+	/* ULL optimization: Prefetch timer structure for restart operations */
+	prefetch(&ts->sched_timer);
+	prefetch(&ts->last_tick);
+#endif
+
 	hrtimer_cancel(&ts->sched_timer);
 	hrtimer_set_expires(&ts->sched_timer, ts->last_tick);
 
diff --git a/kernel/time/timer.c b/kernel/time/timer.c
index 0f8e28abbd7f..778d61257645 100644
--- a/kernel/time/timer.c
+++ b/kernel/time/timer.c
@@ -2346,11 +2346,16 @@ static inline void __run_timers(struct timer_base *base)
 
 	lockdep_assert_held(&base->lock);
 
-	if (base->running_timer)
+	if (unlikely(base->running_timer))
 		return;
 
-	while (time_after_eq(jiffies, base->clk) &&
-	       time_after_eq(jiffies, base->next_expiry)) {
+	/* ULL optimization: prefetch timer collection structures */
+	prefetch(heads);
+	prefetch(&base->vectors);
+	prefetch(&base->next_expiry);
+
+	while (likely(time_after_eq(jiffies, base->clk)) &&
+	       likely(time_after_eq(jiffies, base->next_expiry))) {
 		levels = collect_expired_timers(base, heads);
 		/*
 		 * The two possible reasons for not finding any expired
@@ -2368,17 +2373,25 @@ static inline void __run_timers(struct timer_base *base)
 		base->clk++;
 		timer_recalc_next_expiry(base);
 
-		while (levels--)
+		/* ULL batch processing: prefetch next level during current processing */
+		while (levels--) {
+			if (likely(levels > 0))
+				prefetch(heads + levels - 1);
 			expire_timers(base, heads + levels);
+		}
 	}
 }
 
 static void __run_timer_base(struct timer_base *base)
 {
 	/* Can race against a remote CPU updating next_expiry under the lock */
-	if (time_before(jiffies, READ_ONCE(base->next_expiry)))
+	if (likely(time_before(jiffies, READ_ONCE(base->next_expiry))))
 		return;
 
+	/* ULL optimization: prefetch base structure before lock acquisition */
+	prefetch(&base->lock);
+	prefetch(&base->running_timer);
+
 	timer_base_lock_expiry(base);
 	raw_spin_lock_irq(&base->lock);
 	__run_timers(base);
@@ -2414,9 +2427,19 @@ static __latent_entropy void run_timer_softirq(void)
 static void run_local_timers(void)
 {
 	struct timer_base *base = this_cpu_ptr(&timer_bases[BASE_LOCAL]);
+	bool need_softirq = false;
+
+	/* ULL optimization: prefetch timer base structures */
+	prefetch(this_cpu_ptr(&timer_bases[BASE_LOCAL]));
+	prefetch(this_cpu_ptr(&timer_bases[BASE_GLOBAL]));
 
 	hrtimer_run_queues();
 
+	/*
+	 * ULL batch processing optimization: instead of raising softirq
+	 * immediately on first expired timer, check all bases first to
+	 * minimize interrupt overhead through batching
+	 */
 	for (int i = 0; i < NR_BASES; i++, base++) {
 		/*
 		 * Raise the softirq only if required.
@@ -2451,12 +2474,14 @@ static void run_local_timers(void)
 		 * Possible remote writers are using WRITE_ONCE(). Local reader
 		 * uses therefore READ_ONCE().
 		 */
-		if (time_after_eq(jiffies, READ_ONCE(base->next_expiry)) ||
-		    (i == BASE_DEF && tmigr_requires_handle_remote())) {
-			raise_timer_softirq(TIMER_SOFTIRQ);
-			return;
+		if (likely(time_after_eq(jiffies, READ_ONCE(base->next_expiry)) ||
+		           (i == BASE_DEF && tmigr_requires_handle_remote()))) {
+			need_softirq = true;
 		}
 	}
+
+	if (unlikely(need_softirq))
+		raise_timer_softirq(TIMER_SOFTIRQ);
 }
 
 /*
-- 
2.43.0

sched/deadline: Avoid dl_server boosting with expired deadline
https://lore.kernel.org/all/20251007122904.31611-1-gmonaco@redhat.com/

Recent changes to the deadline server leave it running when the system
is idle. If the system is idle for longer than the dl_server period and
the first scheduling occurs after a fair task wakes up, the algorithm
picks the server as the earliest deadline (in the past) and that boosts
the fair task that just woke up while:
 * the deadline is in the past
 * the server consumed all its runtime (in background)
 * there is no starvation (idle for about a period)

Prevent the server from boosting a task when the deadline is in the
past. Instead, replenish a new period and start the server as deferred.

Fixes: 4ae8d9aa9f9d ("sched/deadline: Fix dl_server getting stuck")
To: Juri Lelli <juri.lelli@xxxxxxxxxx>
Cc: Clark Williams <williams@xxxxxxxxxx>
Signed-off-by: Gabriele Monaco <gmonaco@xxxxxxxxxx>

diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index 72c1f72463c7..b3e3d506a18d 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -2374,6 +2374,17 @@ static struct task_struct *__pick_task_dl(struct rq *rq)
 			dl_server_stop(dl_se);
 			goto again;
 		}
+		/*
+		 * If the CPU was idle for long enough time and wakes up
+		 * because of a fair task, the dl_server may run after its
+		 * period elapsed. Replenish a new period as deferred, since we
+		 * are clearly not handling starvation here.
+		 */
+		if (dl_time_before(dl_se->deadline, rq_clock(rq))) {
+			dl_se->dl_defer_running = 0;
+			replenish_dl_new_period(dl_se, rq);
+			goto again;
+		}
 		rq->dl_server = dl_se;
 	} else {
 		p = dl_task_of(dl_se);
-- 
2.51.0

Fix "sched/core: Tweak wait_task_inactive() to force dequeue sched_delayed tasks"
https://lore.kernel.org/all/20250925133310.1843863-1-matt@readmodwrite.com/
https://lore.kernel.org/all/105ae6f1-f629-4fe7-9644-4242c3bed035@amd.com/

From Matt Fleming:
  If we dequeue a task (task B) that was sched delayed then that task is
  definitely no longer on the rq and not tracked in the rbtree.
  Unfortunately, task_on_rq_queued(B) will still return true because
  dequeue_task() doesn't update p->on_rq.

From John Stultz:
  There are two spots where we might exit dequeue_entities()
  early when cfs_rq_throttled(rq), so maybe that's what's catching us
  here?

From K Prateek Nayak:
  That could very likely be it.

  Matt, if possible can you try the patch attached below to check if the
  bailout for throttled hierarchy is indeed the root cause. Thanks in
  advance.

From Matt Fleming:
  I've been running our reproducer with this patch for the last few
  hours without any issues, so the fix looks good to me.

Tested-by: Matt Fleming <mfleming@xxxxxxxxxx>

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 8ce56a8d507f..f0a4d9d7424d 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -6982,6 +6982,7 @@ static int dequeue_entities(struct rq *rq, struct sched_entity *se, int flags)
 	int h_nr_runnable = 0;
 	struct cfs_rq *cfs_rq;
 	u64 slice = 0;
+	int ret = 0; /* XXX: Do we care if ret is 0 vs 1 since we only check ret < 0? */
 
 	if (entity_is_task(se)) {
 		p = task_of(se);
@@ -7011,7 +7012,7 @@ static int dequeue_entities(struct rq *rq, struct sched_entity *se, int flags)
 
 		/* end evaluation on encountering a throttled cfs_rq */
 		if (cfs_rq_throttled(cfs_rq))
-			return 0;
+			goto out;
 
 		/* Don't dequeue parent if it has other entities besides us */
 		if (cfs_rq->load.weight) {
@@ -7052,7 +7053,7 @@ static int dequeue_entities(struct rq *rq, struct sched_entity *se, int flags)
 
 		/* end evaluation on encountering a throttled cfs_rq */
 		if (cfs_rq_throttled(cfs_rq))
-			return 0;
+			goto out;
 	}
 
 	sub_nr_running(rq, h_nr_queued);
@@ -7061,6 +7062,8 @@ static int dequeue_entities(struct rq *rq, struct sched_entity *se, int flags)
 	if (unlikely(!was_sched_idle && sched_idle_rq(rq)))
 		rq->next_balance = jiffies;
 
+	ret = 1;
+out:
 	if (p && task_delayed) {
 		WARN_ON_ONCE(!task_sleep);
 		WARN_ON_ONCE(p->on_rq != 1);
@@ -7076,7 +7079,7 @@ static int dequeue_entities(struct rq *rq, struct sched_entity *se, int flags)
 		__block_task(rq, p);
 	}
 
-	return 1;
+	return ret;
 }
 
 /*
-- 
2.50.1

sched/fair: do not scan twice in detach_tasks()
https://lore.kernel.org/all/20250722102600.25976-1-shijie@os.amperecomputing.com/

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 7cc9d50e3e11..9c1f21d59b5c 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -11712,12 +11712,15 @@ static int sched_balance_rq(int this_cpu, struct rq *this_rq,
 		 * still unbalanced. ld_moved simply stays zero, so it is
 		 * correctly treated as an imbalance.
 		 */
-		env.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);
-
 more_balance:
 		rq_lock_irqsave(busiest, &rf);
 		update_rq_clock(busiest);
 
+		if (!env.loop_max)
+			env.loop_max  = min(sysctl_sched_nr_migrate, busiest->cfs.h_nr_queued);
+		else
+			env.loop_max  = min(env.loop_max, busiest->cfs.h_nr_queued);
+
 		/*
 		 * cur_ld_moved - load moved in current iteration
 		 * ld_moved     - cumulative load moved across iterations
-- 
2.40.1

Bluetooth adapters of many Acer laptops don't work without it

diff -uarp a/drivers/bluetooth/btusb.c b/drivers/bluetooth/btusb.c
--- a/drivers/bluetooth/btusb.c
+++ b/drivers/bluetooth/btusb.c
@@ -574,6 +574,8 @@ static const struct usb_device_id quirks
 						     BTUSB_WIDEBAND_SPEECH },
 	{ USB_DEVICE(0x13d3, 0x3591), .driver_info = BTUSB_REALTEK |
 						     BTUSB_WIDEBAND_SPEECH },
+	{ USB_DEVICE(0x13d3, 0x3601), .driver_info = BTUSB_REALTEK |
+						     BTUSB_WIDEBAND_SPEECH },
 	{ USB_DEVICE(0x13d3, 0x3618), .driver_info = BTUSB_REALTEK |
 						     BTUSB_WIDEBAND_SPEECH },
 	{ USB_DEVICE(0x0489, 0xe123), .driver_info = BTUSB_REALTEK |
-- 
2.40.2

Add Logitech C310

diff --git a/drivers/usb/core/quirks.c b/drivers/usb/core/quirks.c
index 0cf94c7a2c9c..20450f7a0c92 100644
--- a/drivers/usb/core/quirks.c
+++ b/drivers/usb/core/quirks.c
@@ -268,6 +268,9 @@ static const struct usb_device_id usb_quirk_list[] = {
 	/* Logitech Harmony 700-series */
 	{ USB_DEVICE(0x046d, 0xc122), .driver_info = USB_QUIRK_DELAY_INIT },
 
+	/* Logitech Webcam C310 */
+	{ USB_DEVICE(0x046d, 0x081b), .driver_info = USB_QUIRK_RESET_RESUME },
+
 	/* Philips PSC805 audio device */
 	{ USB_DEVICE(0x0471, 0x0155), .driver_info = USB_QUIRK_RESET_RESUME },
 
-- 
2.40.2

processor_idle: Skip dummy wait for processors based on Zen microarchitecture
https://lore.kernel.org/all/20220921063638.2489-1-kprateek.nayak@amd.com/

diff --git a/drivers/acpi/processor_idle.c b/drivers/acpi/processor_idle.c
index 2c2dc559e0f8de..48d6b137e8f6ae 100644
--- a/drivers/acpi/processor_idle.c
+++ b/drivers/acpi/processor_idle.c
@@ -524,8 +524,11 @@ static __cpuidle void io_idle(unsigned long addr)
 	inb(addr);
 
 #ifdef	CONFIG_X86
-	/* No delay is needed if we are in guest */
-	if (boot_cpu_has(X86_FEATURE_HYPERVISOR))
+	/*
+	 * No delay is needed if we are in guest or on a processor
+	 * based on the Zen microarchitecture.
+	 */
+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR) || boot_cpu_has(X86_FEATURE_ZEN))
 		return;
 	/*
 	 * Modern (>=Nehalem) Intel systems use ACPI via intel_idle,
-- 
2.40.2

ZEN: mm: Stop kswapd early when nothing's waiting for it to free pages
https://github.com/torvalds/linux/commit/def0b563099bd1c8a6c359334d8f0d0b43935388

Contains:
  - mm: Stop kswapd early when nothing's waiting for it to free pages

    Keeping kswapd running when all the failed allocations that invoked it
    are satisfied incurs a high overhead due to unnecessary page eviction
    and writeback, as well as spurious VM pressure events to various
    registered shrinkers. When kswapd doesn't need to work to make an
    allocation succeed anymore, stop it prematurely to save resources.

    Signed-off-by: Sultan Alsawaf <sultan@xxxxxxxxxx>

  - mm: Don't stop kswapd on a per-node basis when there are no waiters

    The page allocator wakes all kswapds in an allocation context's allowed
    nodemask in the slow path, so it doesn't make sense to have the kswapd-
    waiter count per each NUMA node. Instead, it should be a global counter
    to stop all kswapds when there are no failed allocation requests.

    Signed-off-by: Sultan Alsawaf <sultan@xxxxxxxxxx>

  - mm: Increment kswapd_waiters for throttled direct reclaimers

    Throttled direct reclaimers will wake up kswapd and wait for kswapd to
    satisfy their page allocation request, even when the failed allocation
    lacks the __GFP_KSWAPD_RECLAIM flag in its gfp mask. As a result, kswapd
    may think that there are no waiters and thus exit prematurely, causing
    throttled direct reclaimers lacking __GFP_KSWAPD_RECLAIM to stall on
    waiting for kswapd to wake them up. Incrementing the kswapd_waiters
    counter when such direct reclaimers become throttled fixes the problem.

    Signed-off-by: Sultan Alsawaf <sultan@xxxxxxxxxx>
---
 mm/internal.h   |  1 +
 mm/page_alloc.c | 17 ++++++++++++++---
 mm/vmscan.c     | 19 +++++++++++++------
 3 files changed, 28 insertions(+), 9 deletions(-)

diff --git a/mm/internal.h b/mm/internal.h
index 45b725c3dc030c..fa0ded5bc5da68 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -822,6 +822,7 @@ void post_alloc_hook(struct page *page, unsigned int order, gfp_t gfp_flags);
 extern bool free_pages_prepare(struct page *page, unsigned int order);
 
 extern int user_min_free_kbytes;
+extern atomic_long_t kswapd_waiters;
 
 struct page *__alloc_frozen_pages_noprof(gfp_t, unsigned int order, int nid,
 		nodemask_t *);
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 0dd4e419989a60..ea6d869c3b98b4 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -91,6 +91,8 @@ typedef int __bitwise fpi_t;
 /* Free the page without taking locks. Rely on trylock only. */
 #define FPI_TRYLOCK		((__force fpi_t)BIT(2))
 
+atomic_long_t kswapd_waiters = ATOMIC_LONG_INIT(0);
+
 /* prevent >1 _updater_ of zone percpu pageset ->high and ->batch fields */
 static DEFINE_MUTEX(pcp_batch_high_lock);
 #define MIN_PERCPU_PAGELIST_HIGH_FRACTION (8)
@@ -4614,6 +4616,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
 	unsigned int cpuset_mems_cookie;
 	unsigned int zonelist_iter_cookie;
 	int reserve_flags;
+	bool woke_kswapd = false;
 
 	if (unlikely(nofail)) {
 		/*
@@ -4673,8 +4676,13 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
 			goto nopage;
 	}
 
-	if (alloc_flags & ALLOC_KSWAPD)
+	if (alloc_flags & ALLOC_KSWAPD) {
+		if (!woke_kswapd) {
+			atomic_long_inc(&kswapd_waiters);
+			woke_kswapd = true;
+		}
 		wake_all_kswapds(order, gfp_mask, ac);
+	}
 
 	/*
 	 * The adjusted alloc_flags might result in immediate success, so try
@@ -4889,9 +4897,12 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
 		goto retry;
 	}
 fail:
-	warn_alloc(gfp_mask, ac->nodemask,
-			"page allocation failure: order:%u", order);
 got_pg:
+	if (woke_kswapd)
+		atomic_long_dec(&kswapd_waiters);
+	if (!page)
+		warn_alloc(gfp_mask, ac->nodemask,
+				"page allocation failure: order:%u", order);
 	return page;
 }
 
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 674999999cd067..24569fe41c4ff6 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -6632,7 +6632,7 @@ static unsigned long do_try_to_free_pages(struct zonelist *zonelist,
 	return 0;
 }
 
-static bool allow_direct_reclaim(pg_data_t *pgdat)
+static bool allow_direct_reclaim(pg_data_t *pgdat, bool using_kswapd)
 {
 	struct zone *zone;
 	unsigned long pfmemalloc_reserve = 0;
@@ -6657,6 +6657,10 @@ static bool allow_direct_reclaim(pg_data_t *pgdat)
 
 	wmark_ok = free_pages > pfmemalloc_reserve / 2;
 
+	/* The throttled direct reclaimer is now a kswapd waiter */
+	if (unlikely(!using_kswapd && !wmark_ok))
+		atomic_long_inc(&kswapd_waiters);
+
 	/* kswapd must be awake if processes are being throttled */
 	if (!wmark_ok && waitqueue_active(&pgdat->kswapd_wait)) {
 		if (READ_ONCE(pgdat->kswapd_highest_zoneidx) > ZONE_NORMAL)
@@ -6722,7 +6726,7 @@ static bool throttle_direct_reclaim(gfp_t gfp_mask, struct zonelist *zonelist,
 
 		/* Throttle based on the first usable node */
 		pgdat = zone->zone_pgdat;
-		if (allow_direct_reclaim(pgdat))
+		if (allow_direct_reclaim(pgdat, gfp_mask & __GFP_KSWAPD_RECLAIM))
 			goto out;
 		break;
 	}
@@ -6744,11 +6748,14 @@ static bool throttle_direct_reclaim(gfp_t gfp_mask, struct zonelist *zonelist,
 	 */
 	if (!(gfp_mask & __GFP_FS))
 		wait_event_interruptible_timeout(pgdat->pfmemalloc_wait,
-			allow_direct_reclaim(pgdat), HZ);
+			allow_direct_reclaim(pgdat, true), HZ);
 	else
 		/* Throttle until kswapd wakes the process */
 		wait_event_killable(zone->zone_pgdat->pfmemalloc_wait,
-			allow_direct_reclaim(pgdat));
+			allow_direct_reclaim(pgdat, true));
+
+	if (unlikely(!(gfp_mask & __GFP_KSWAPD_RECLAIM)))
+		atomic_long_dec(&kswapd_waiters);
 
 	if (fatal_signal_pending(current))
 		return true;
@@ -7278,14 +7285,14 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)
 		 * able to safely make forward progress. Wake them
 		 */
 		if (waitqueue_active(&pgdat->pfmemalloc_wait) &&
-				allow_direct_reclaim(pgdat))
+				allow_direct_reclaim(pgdat, true))
 			wake_up_all(&pgdat->pfmemalloc_wait);
 
 		/* Check if kswapd should be suspending */
 		__fs_reclaim_release(_THIS_IP_);
 		ret = kthread_freezable_should_stop(&was_frozen);
 		__fs_reclaim_acquire(_THIS_IP_);
-		if (was_frozen || ret)
+		if (was_frozen || ret || !atomic_long_read(&kswapd_waiters))
 			break;
 
 		/*
-- 
2.40.2

Subject: [PATCH] HID: magicmouse: fix regression breaking support for Magic
 Trackpad 1

The case HID_ANY_ID and default are technically the same, but the first
one was assigning no report to the Magic Trackpad 1, while the second
one assigns the correct report. Since the first case is matched first,
the Magic Trackpad 1 was not being assigned any report, breaking
support for it.

Signed-off-by: Aditya Garg <gargaditya08@xxxxxxxxxx>
---
 drivers/hid/hid-magicmouse.c | 15 ++++++---------
 1 file changed, 6 insertions(+), 9 deletions(-)

diff --git a/drivers/hid/hid-magicmouse.c b/drivers/hid/hid-magicmouse.c
index b40f41168..4bf49d7ca 100644
--- a/drivers/hid/hid-magicmouse.c
+++ b/drivers/hid/hid-magicmouse.c
@@ -1589,7 +1589,7 @@ static int magicmouse_probe(struct hid_device *hdev,
 		report = hid_register_report(hdev, HID_INPUT_REPORT,
 			TRACKPAD2_USB_REPORT_ID, 0);
 		break;
-	case HID_ANY_ID:
+	default:
 		switch (id->bus) {
 		case BUS_HOST:
 			report = hid_register_report(hdev, HID_INPUT_REPORT, MTP_REPORT_ID, 0);
@@ -1597,15 +1597,12 @@ static int magicmouse_probe(struct hid_device *hdev,
 		case BUS_SPI:
 			report = hid_register_report(hdev, HID_INPUT_REPORT, SPI_REPORT_ID, 0);
 			break;
-		default:
-			break;
+		default: /* USB_DEVICE_ID_APPLE_MAGICTRACKPAD */
+			report = hid_register_report(hdev, HID_INPUT_REPORT,
+				TRACKPAD_REPORT_ID, 0);
+			report = hid_register_report(hdev, HID_INPUT_REPORT,
+				DOUBLE_REPORT_ID, 0);
 		}
-		break;
-	default: /* USB_DEVICE_ID_APPLE_MAGICTRACKPAD */
-		report = hid_register_report(hdev, HID_INPUT_REPORT,
-			TRACKPAD_REPORT_ID, 0);
-		report = hid_register_report(hdev, HID_INPUT_REPORT,
-			DOUBLE_REPORT_ID, 0);
 	}
 
 	if (!report) {
-- 
2.50.1

Subject: [PATCH] x86,retpoline: Optimize patch_retpoline()
https://lore.kernel.org/all/175757609204.709179.6413279343921603547.tip-bot2@tip-bot2/

Currently the very common retpoline: "CS CALL __x86_indirect_thunk_r11"
is transformed into "CALL *R11; NOP3" for eIBRS/BHI_NO parts.

Similarly, paranoid fineibt has: "CALL *R11; NOP".

Recognise that CS stuffing can avoid the extra NOP. However, due to
prefix decode penalties, make sure to not emit too many CS prefixes.
Notably: "CS CALL __x86_indirect_thunk_rax" must not become "CS CS CS
CS CALL *RAX". Prefix decode penalties are typically many more cycles
than decoding an extra NOP.

Additionally, if the retpoline is a tail-call, the "JMP *%\reg" should
be followed by INT3 for straight-line-speculation mitigation, since
emit_indirect() now has a length argument, move this into
emit_indirect() such that other users (paranoid-fineibt) also do this.

Signed-off-by: Peter Zijlstra (Intel) <peterz@xxxxxxxxxx>
Link: https://lkml.kernel.org/r/20250902104627.GM4068168@noisy.programming.kicks-ass.net
---
 arch/x86/kernel/alternative.c | 42 ++++++++++++++++++++++-------------
 1 file changed, 26 insertions(+), 16 deletions(-)

diff --git a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c
index 7bde68247b5fc5..18584a30a75e9f 100644
--- a/arch/x86/kernel/alternative.c
+++ b/arch/x86/kernel/alternative.c
@@ -713,20 +713,33 @@ static inline bool is_jcc32(struct insn *insn)
 #if defined(CONFIG_MITIGATION_RETPOLINE) && defined(CONFIG_OBJTOOL)
 
 /*
- * CALL/JMP *%\reg
+ * [CS]{,3} CALL/JMP *%\reg [INT3]*
  */
-static int emit_indirect(int op, int reg, u8 *bytes)
+static int emit_indirect(int op, int reg, u8 *bytes, int len)
 {
+	int cs = 0, bp = 0;
 	int i = 0;
 	u8 modrm;
 
+	/*
+	 * Set @len to the excess bytes after writing the instruction.
+	 */
+	len -= 2 + (reg >= 8);
+	WARN_ON_ONCE(len < 0);
+
 	switch (op) {
 	case CALL_INSN_OPCODE:
 		modrm = 0x10; /* Reg = 2; CALL r/m */
+		/*
+		 * Additional NOP is better than prefix decode penalty.
+		 */
+		if (len <= 3)
+			cs = len;
 		break;
 
 	case JMP32_INSN_OPCODE:
 		modrm = 0x20; /* Reg = 4; JMP r/m */
+		bp = len;
 		break;
 
 	default:
@@ -734,6 +747,9 @@ static int emit_indirect(int op, int reg, u8 *bytes)
 		return -1;
 	}
 
+	while (cs--)
+		bytes[i++] = 0x2e; /* CS-prefix */
+
 	if (reg >= 8) {
 		bytes[i++] = 0x41; /* REX.B prefix */
 		reg -= 8;
@@ -745,6 +761,9 @@ static int emit_indirect(int op, int reg, u8 *bytes)
 	bytes[i++] = 0xff; /* opcode */
 	bytes[i++] = modrm;
 
+	while (bp--)
+		bytes[i++] = 0xcc; /* INT3 */
+
 	return i;
 }
 
@@ -918,20 +937,11 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 		return emit_its_trampoline(addr, insn, reg, bytes);
 #endif
 
-	ret = emit_indirect(op, reg, bytes + i);
+	ret = emit_indirect(op, reg, bytes + i, insn->length - i);
 	if (ret < 0)
 		return ret;
 	i += ret;
 
-	/*
-	 * The compiler is supposed to EMIT an INT3 after every unconditional
-	 * JMP instruction due to AMD BTC. However, if the compiler is too old
-	 * or MITIGATION_SLS isn't enabled, we still need an INT3 after
-	 * indirect JMPs even on Intel.
-	 */
-	if (op == JMP32_INSN_OPCODE && i < insn->length)
-		bytes[i++] = INT3_INSN_OPCODE;
-
 	for (; i < insn->length;)
 		bytes[i++] = BYTES_NOP1;
 
@@ -1407,8 +1417,7 @@ asm(	".pushsection .rodata				\n"
 	"	lea	-0x10(%r11), %r11		\n"
 	"	jne	fineibt_paranoid_start+0xd	\n"
 	"fineibt_paranoid_ind:				\n"
-	"	call	*%r11				\n"
-	"	nop					\n"
+	"	cs call	*%r11				\n"
 	"fineibt_paranoid_end:				\n"
 	".popsection					\n"
 );
@@ -1696,8 +1705,9 @@ static int cfi_rewrite_callers(s32 *start, s32 *end)
 			emit_paranoid_trampoline(addr + fineibt_caller_size,
 						 &insn, 11, bytes + fineibt_caller_size);
 		} else {
-			ret = emit_indirect(op, 11, bytes + fineibt_paranoid_ind);
-			if (WARN_ON_ONCE(ret != 3))
+			int len = fineibt_paranoid_size - fineibt_paranoid_ind;
+			ret = emit_indirect(op, 11, bytes + fineibt_paranoid_ind, len);
+			if (WARN_ON_ONCE(ret != len))
 				continue;
 		}
 
-- 
2.50.1

Subject: [PATCH] x86/mm: Change cpa_flush() to call flush_kernel_range()
 directly
https://lore.kernel.org/all/175588460052.1420.3714146338769865045.tip-bot2@tip-bot2/

The function cpa_flush() calls __flush_tlb_one_kernel() and
flush_tlb_all().

Replacing that with a call to flush_tlb_kernel_range() allows
cpa_flush() to make use of INVLPGB or RAR without any additional
changes.

Initialize invlpgb_count_max to 1, since flush_tlb_kernel_range()
can now be called before invlpgb_count_max has been initialized
to the value read from CPUID.

[riel: remove now unused __cpa_flush_tlb]

Signed-off-by: Yu-cheng Yu <yu-cheng.yu@xxxxxxxxxx>
Signed-off-by: Rik van Riel <riel@xxxxxxxxxx>
Signed-off-by: Dave Hansen <dave.hansen@xxxxxxxxxx>
Acked-by: Dave Hansen <dave.hansen@xxxxxxxxxx>
Link: https://lore.kernel.org/all/20250606171112.4013261-4-riel%40surriel.com
---
 arch/x86/mm/pat/set_memory.c | 20 +++++++-------------
 1 file changed, 7 insertions(+), 13 deletions(-)

diff --git a/arch/x86/mm/pat/set_memory.c b/arch/x86/mm/pat/set_memory.c
index 8834c76f91c9e2..d2d54b8c4dbb04 100644
--- a/arch/x86/mm/pat/set_memory.c
+++ b/arch/x86/mm/pat/set_memory.c
@@ -399,15 +399,6 @@ static void cpa_flush_all(unsigned long cache)
 	on_each_cpu(__cpa_flush_all, (void *) cache, 1);
 }
 
-static void __cpa_flush_tlb(void *data)
-{
-	struct cpa_data *cpa = data;
-	unsigned int i;
-
-	for (i = 0; i < cpa->numpages; i++)
-		flush_tlb_one_kernel(fix_addr(__cpa_addr(cpa, i)));
-}
-
 static int collapse_large_pages(unsigned long addr, struct list_head *pgtables);
 
 static void cpa_collapse_large_pages(struct cpa_data *cpa)
@@ -444,6 +435,7 @@ static void cpa_collapse_large_pages(struct cpa_data *cpa)
 
 static void cpa_flush(struct cpa_data *cpa, int cache)
 {
+	unsigned long start, end;
 	unsigned int i;
 
 	BUG_ON(irqs_disabled() && !early_boot_irqs_disabled);
@@ -453,10 +445,12 @@ static void cpa_flush(struct cpa_data *cpa, int cache)
 		goto collapse_large_pages;
 	}
 
-	if (cpa->force_flush_all || cpa->numpages > tlb_single_page_flush_ceiling)
-		flush_tlb_all();
-	else
-		on_each_cpu(__cpa_flush_tlb, cpa, 1);
+	start = fix_addr(__cpa_addr(cpa, 0));
+	end =   fix_addr(__cpa_addr(cpa, cpa->numpages));
+	if (cpa->force_flush_all)
+		end = TLB_FLUSH_ALL;
+
+	flush_tlb_kernel_range(start, end);
 
 	if (!cache)
 		goto collapse_large_pages;
-- 
2.50.1

Subject: [PATCH] perf/x86/amd/uncore: use kcalloc() instead of multiplication
https://lore.kernel.org/all/455fb1db8ab0811d2336e0ec198c728a0c703be9.1757744812.git.joeypabalinas@gmail.com/

Dynamic size calculations should not be performed in allocator
function arguments due to overflow risk.

Use kcalloc() instead of multiplication in the first argument
of kzalloc().

Signed-off-by: Joey Pabalinas <joeypabalinas@xxxxxxxxxx>
Reviewed-by: Dapeng Mi <dapeng1.mi@xxxxxxxxxx>
---
 arch/x86/events/amd/uncore.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/arch/x86/events/amd/uncore.c b/arch/x86/events/amd/uncore.c
index e8b6af199c738eb00b..d08e3054461f2ca07a 100644
--- a/arch/x86/events/amd/uncore.c
+++ b/arch/x86/events/amd/uncore.c
@@ -1036,11 +1036,11 @@ int amd_uncore_umc_ctx_init(struct amd_uncore *uncore, unsigned int cpu)
 		group_num_pmus[gid] = hweight32(info.split.aux_data);
 		group_num_pmcs[gid] = info.split.num_pmcs;
 		uncore->num_pmus += group_num_pmus[gid];
 	}
 
-	uncore->pmus = kzalloc(sizeof(*uncore->pmus) * uncore->num_pmus,
+	uncore->pmus = kcalloc(uncore->num_pmus, sizeof(*uncore->pmus),
 			       GFP_KERNEL);
 	if (!uncore->pmus) {
 		uncore->num_pmus = 0;
 		goto done;
 	}
-- 
2.50.1

Subject: Bug: Performance regression in 1013af4f585f:
  mm/hugetlb: fix huge_pmd_unshare() vs GUP-fast race
https://lore.kernel.org/all/4d3878531c76479d9f8ca9789dc6485d@amazon.de/#t

Revert mm/hugetlb: fix huge_pmd_unshare() vs GUP-fast race

diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 7ba020d489d456..8746ed2fec135b 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -7606,13 +7606,6 @@ int huge_pmd_unshare(struct mm_struct *mm, struct vm_area_struct *vma,
 		return 0;
 
 	pud_clear(pud);
-	/*
-	 * Once our caller drops the rmap lock, some other process might be
-	 * using this page table as a normal, non-hugetlb page table.
-	 * Wait for pending gup_fast() in other threads to finish before letting
-	 * that happen.
-	 */
-	tlb_remove_table_sync_one();
 	ptdesc_pmd_pts_dec(virt_to_ptdesc(ptep));
 	mm_dec_nr_pmds(mm);
 	return 1;
-- 
2.50.1

Subject: mm: mprotect: always skip dma pinned folio in prot_numa_skip()
https://lore.kernel.org/all/20251015123516.2703660-2-wangkefeng.wang@huawei.com/

diff --git a/mm/mprotect.c b/mm/mprotect.c
index 113b48985834..bb59a42809b8 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -136,9 +136,12 @@ static bool prot_numa_skip(struct vm_area_struct *vma, unsigned long addr,
 	if (folio_is_zone_device(folio) || folio_test_ksm(folio))
 		goto skip;
 
-	/* Also skip shared copy-on-write pages */
-	if (is_cow_mapping(vma->vm_flags) &&
-	    (folio_maybe_dma_pinned(folio) || folio_maybe_mapped_shared(folio)))
+	/* Also skip shared copy-on-write folios */
+	if (is_cow_mapping(vma->vm_flags) && folio_maybe_mapped_shared(folio))
+		goto skip;
+
+	/* Folios are pinned and can't be migrated */
+	if (folio_maybe_dma_pinned(folio))
 		goto skip;
 
 	/*
-- 
2.27.0


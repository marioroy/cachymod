
Few updates for improving the EEVDF desktop experience

Signed-off-by: Mario Roy <...>


The CachyOS default migration_cost with EEVDF is a little extreme
Bump the value to mitigate stutters, helpful with many cores CPU

diff -uarp a/kernel/sched/fair.c b/kernel/sched/fair.c
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -85,7 +85,7 @@ static unsigned int normalized_sysctl_sched_base_slice
 #endif /* CONFIG_CACHY */
 
 #ifdef CONFIG_CACHY
-__read_mostly unsigned int sysctl_sched_migration_cost	= 300000UL;
+__read_mostly unsigned int sysctl_sched_migration_cost	= 350000UL;
 #else
 __read_mostly unsigned int sysctl_sched_migration_cost	= 500000UL;
 #endif
-- 
2.40.2

sched/fair: use same CFS bandwidth slice as BORE

diff -uarp a/kernel/sched/fair.c b/kernel/sched/fair.c
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -133,12 +133,8 @@ int __weak arch_asym_cpu_priority(int cp
  *
  * (default: 5 msec, units: microseconds)
  */
-#ifdef CONFIG_CACHY
-static unsigned int sysctl_sched_cfs_bandwidth_slice		= 3000UL;
-#else
 static unsigned int sysctl_sched_cfs_bandwidth_slice		= 5000UL;
-#endif
 #endif

 #ifdef CONFIG_NUMA_BALANCING
 /* Restrict the NUMA promotion throughput (MB/s) for each target node. */
-- 
2.40.2

sched/fair: remove upper limit on cpu number, as done in Clear Linux

diff -uarp a/kernel/sched/fair.c b/kernel/sched/fair.c
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -202,7 +202,7 @@ static inline void update_load_set(struct load_weight *lw, unsigned long w)
  */
 static unsigned int get_update_sysctl_factor(void)
 {
-	unsigned int cpus = min_t(unsigned int, num_online_cpus(), 8);
+	unsigned int cpus = num_online_cpus();
 	unsigned int factor;
 
 	switch (sysctl_sched_tunable_scaling) {
-- 
2.40.2

sched/fair: improve smoothness

diff -uarp a/kernel/sched/fair.c b/kernel/sched/fair.c
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -709,6 +709,9 @@ static void update_entity_lag(struct cfs_rq *cfs_rq, struct sched_entity *se)
 
 	vlag = avg_vruntime(cfs_rq) - se->vruntime;
 	limit = calc_delta_fair(max_t(u64, 2*se->slice, TICK_NSEC), se);
+#ifdef CONFIG_CACHY
+	limit >>= 1;
+#endif
 
 	se->vlag = clamp(vlag, -limit, limit);
 }
@@ -948,7 +951,12 @@ static struct sched_entity *pick_eevdf(s
 	if (curr && (!curr->on_rq || !entity_eligible(cfs_rq, curr)))
 		curr = NULL;
 
+#ifdef CONFIG_CACHY
+	if (sched_feat(RUN_TO_PARITY) && curr && protect_slice(curr) &&
+		(!entity_is_task(curr) || cfs_rq->nr_queued <= 3))
+#else
 	if (sched_feat(RUN_TO_PARITY) && curr && protect_slice(curr))
+#endif
 		return curr;
 
 	/* Pick the leftmost entity if it's eligible */
-- 
2.40.2

sched/fair: improve wakeup performance

diff -uarp a/kernel/sched/fair.c b/kernel/sched/fair.c
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5318,7 +5318,11 @@ place_entity(struct cfs_rq *cfs_rq, stru
 	 * on average, halfway through their slice, as such start tasks
 	 * off with half a slice to ease into the competition.
 	 */
+#ifdef CONFIG_CACHY
+	if (sched_feat(PLACE_DEADLINE_INITIAL) && (flags & (ENQUEUE_INITIAL | ENQUEUE_WAKEUP)))
+#else
 	if (sched_feat(PLACE_DEADLINE_INITIAL) && (flags & ENQUEUE_INITIAL))
+#endif
 		vslice /= 2;
 
 	/*
-- 
2.40.2


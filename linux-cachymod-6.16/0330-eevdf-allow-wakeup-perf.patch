
Few updates for improving the EEVDF-BORE desktop experience
Plus drop BORE 6.x features that decrease performance or cause jitters

Signed-off-by: Mario Roy <...>


sched/fair: allow smoothing and wakeup performance with pseudo-EEVDF

diff -uarp a/kernel/sched/bore.c b/kernel/sched/bore.c
--- a/kernel/sched/bore.c
+++ b/kernel/sched/bore.c
@@ -12,7 +12,7 @@ u8   __read_mostly sched_bore
 u8   __read_mostly sched_burst_inherit_type     = 2;
 u8   __read_mostly sched_burst_smoothness       = 1;
 u8   __read_mostly sched_burst_penalty_offset   = 24;
-uint __read_mostly sched_burst_penalty_scale    = 1536;
+uint __read_mostly sched_burst_penalty_scale    = 1280;
 uint __read_mostly sched_burst_cache_lifetime   = 75000000;
 static int __maybe_unused maxval_prio    =   39;
 static int __maybe_unused maxval_6_bits  =   63;
diff -uarp a/kernel/sched/fair.c b/kernel/sched/fair.c
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -725,9 +725,9 @@ static void update_entity_lag(struct cfs
 
 	vlag = avg_vruntime(cfs_rq) - se->vruntime;
 	limit = calc_delta_fair(max_t(u64, 2*se->slice, TICK_NSEC), se);
-#ifdef CONFIG_SCHED_BORE
-	limit >>= !!sched_bore;
-#endif /* CONFIG_SCHED_BORE */
+#ifdef CONFIG_CACHY
+	limit >>= 1;
+#endif
 
 	se->vlag = clamp(vlag, -limit, limit);
 }
@@ -5343,17 +5343,17 @@ place_entity(struct cfs_rq *cfs_rq, stru
 		goto vslice_found;
 #endif /* !CONFIG_SCHED_BORE */
 	vslice = calc_delta_fair(se->slice, se);
-#ifdef CONFIG_SCHED_BORE
-	if (likely(sched_bore))
-		vslice >>= !!(flags & (ENQUEUE_INITIAL | ENQUEUE_WAKEUP));
-	else
-#endif /* CONFIG_SCHED_BORE */
+
 	/*
 	 * When joining the competition; the existing tasks will be,
 	 * on average, halfway through their slice, as such start tasks
 	 * off with half a slice to ease into the competition.
 	 */
+#ifdef CONFIG_CACHY
+	if (sched_feat(PLACE_DEADLINE_INITIAL) && (flags & (ENQUEUE_INITIAL | ENQUEUE_WAKEUP)))
+#else
 	if (sched_feat(PLACE_DEADLINE_INITIAL) && (flags & ENQUEUE_INITIAL))
+#endif
 		vslice /= 2;
 
 #ifdef CONFIG_SCHED_BORE
-- 
2.40.2

Restore BORE 5.9.6-like performance

sched/fair: Drop BORE futex boost optimization
sched/fair: Drop BORE goto vslice_found logic
sched/fair: Drop BORE RUN_TO_PARITY_BORE
sched/fair: Modify RUN_TO_PARITY

diff -uarp a/include/linux/sched.h b/include/linux/sched.h
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -832,7 +832,6 @@ struct bore_ctx {
 		};
 	};
 	bool			stop_update;
-	bool			futex_waiting;
 };
 #endif /* CONFIG_SCHED_BORE */
 
diff -uarp a/kernel/futex/waitwake.c b/kernel/futex/waitwake.c
--- a/kernel/futex/waitwake.c
+++ b/kernel/futex/waitwake.c
@@ -358,15 +358,7 @@ void futex_do_wait(struct futex_q *q, st
 		 * is no timeout, or if it has yet to expire.
 		 */
 		if (!timeout || timeout->task)
-#ifdef CONFIG_SCHED_BORE
-		{
-			current->bore.futex_waiting = true;
-#endif /* CONFIG_SCHED_BORE */
 			schedule();
-#ifdef CONFIG_SCHED_BORE
-			current->bore.futex_waiting = false;
-		}
-#endif /* CONFIG_SCHED_BORE */
 	}
 	__set_current_state(TASK_RUNNING);
 }
diff -uarp a/kernel/sched/bore.c b/kernel/sched/bore.c
--- a/kernel/sched/bore.c
+++ b/kernel/sched/bore.c
@@ -262,7 +262,6 @@ void task_fork_bore(struct task_struct *
 	ctx->curr_penalty  = 0;
 	ctx->burst_time    = 0;
 	ctx->stop_update   = false;
-	ctx->futex_waiting = false;
 	update_penalty(p);
 }
 
diff -uarp a/kernel/sched/fair.c b/kernel/sched/fair.c
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -967,16 +967,12 @@ static struct sched_entity *pick_eevdf(s
 	if (curr && (!curr->on_rq || !entity_eligible(cfs_rq, curr)))
 		curr = NULL;
 
-#if !defined(CONFIG_SCHED_BORE)
+#ifdef CONFIG_CACHY
+	if (sched_feat(RUN_TO_PARITY) && curr && protect_slice(curr) &&
+		(!entity_is_task(curr) || cfs_rq->nr_queued <= 3))
+#else
 	if (sched_feat(RUN_TO_PARITY) && curr && protect_slice(curr))
-#else /* CONFIG_SCHED_BORE */
-	bool run_to_parity = likely(sched_bore) ?
-		sched_feat(RUN_TO_PARITY_BORE) : sched_feat(RUN_TO_PARITY);
-	if (run_to_parity && curr && protect_slice(curr) &&
-		(!entity_is_task(curr) ||
-		 !task_of(curr)->bore.futex_waiting ||
-		 unlikely(!sched_bore)))
-#endif /* CONFIG_SCHED_BORE */
+#endif
 		return curr;
 
 	/* Pick the leftmost entity if it's eligible */
@@ -5247,11 +5243,12 @@ void __setparam_fair(struct task_struct
 static void
 place_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
 {
-	u64 vslice = 0, vruntime = avg_vruntime(cfs_rq);
+	u64 vslice, vruntime = avg_vruntime(cfs_rq);
 	s64 lag = 0;
 
 	if (!se->custom_slice)
 		se->slice = sysctl_sched_base_slice;
+	vslice = calc_delta_fair(se->slice, se);
 
 	/*
 	 * Due to how V is constructed as the weighted average of entities,
@@ -5336,13 +5333,6 @@ place_entity(struct cfs_rq *cfs_rq, stru
 		se->rel_deadline = 0;
 		return;
 	}
-#ifdef CONFIG_SCHED_BORE
-	if (entity_is_task(se) &&
-			likely(sched_bore) &&
-			task_of(se)->bore.futex_waiting)
-		goto vslice_found;
-#endif /* !CONFIG_SCHED_BORE */
-	vslice = calc_delta_fair(se->slice, se);
 
 	/*
 	 * When joining the competition; the existing tasks will be,
@@ -5356,9 +5346,6 @@ place_entity(struct cfs_rq *cfs_rq, stru
 #endif
 		vslice /= 2;
 
-#ifdef CONFIG_SCHED_BORE
-vslice_found:
-#endif /* CONFIG_SCHED_BORE */
 	/*
 	 * EEVDF: vd_i = ve_i + r_i/w_i
 	 */
diff -uarp a/kernel/sched/features.h b/kernel/sched/features.h
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@ -18,9 +18,6 @@ SCHED_FEAT(PLACE_REL_DEADLINE, true)
  * 0-lag point or until is has exhausted it's slice.
  */
 SCHED_FEAT(RUN_TO_PARITY, true)
-#ifdef CONFIG_SCHED_BORE
-SCHED_FEAT(RUN_TO_PARITY_BORE, false)
-#endif /* CONFIG_SCHED_BORE */
 /*
  * Allow wakeup of tasks with a shorter slice to cancel RUN_TO_PARITY for
  * current.
-- 
2.40.2

sched/fair: Drop BORE ENQUEUE_WAKEUP at requeue to mitigate jitters

diff -uarp a/kernel/sched/fair.c b/kernel/sched/fair.c
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5356,7 +5356,7 @@ static void check_enqueue_throttle(struc
 static inline int cfs_rq_throttled(struct cfs_rq *cfs_rq);
 
 static void
-requeue_delayed_entity(struct sched_entity *se, int flags);
+requeue_delayed_entity(struct sched_entity *se);
 
 static void
 enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
@@ -5520,10 +5520,6 @@ dequeue_entity(struct cfs_rq *cfs_rq, st
 		if (sched_feat(DELAY_DEQUEUE) && delay &&
 		    !entity_eligible(cfs_rq, se)) {
 			update_load_avg(cfs_rq, se, 0);
-#ifdef CONFIG_SCHED_BORE
-			if (sched_feat(DELAY_ZERO) && likely(sched_bore))
-				update_entity_lag(cfs_rq, se);
-#endif /* CONFIG_SCHED_BORE */
 			set_delayed(se);
 			return false;
 		}
@@ -6935,7 +6931,7 @@ static int sched_idle_cpu(int cpu)
 #endif
 
 static void
-requeue_delayed_entity(struct sched_entity *se, int flags)
+requeue_delayed_entity(struct sched_entity *se)
 {
 	struct cfs_rq *cfs_rq = cfs_rq_of(se);
 
@@ -6948,22 +6944,13 @@ requeue_delayed_entity(struct sched_enti
 	WARN_ON_ONCE(!se->on_rq);
 
 	if (sched_feat(DELAY_ZERO)) {
-#ifdef CONFIG_SCHED_BORE
-		if (likely(sched_bore))
-			flags |= ENQUEUE_WAKEUP;
-		else {
-#endif /* CONFIG_SCHED_BORE */
-		flags = 0;
 		update_entity_lag(cfs_rq, se);
-#ifdef CONFIG_SCHED_BORE
-		}
-#endif /* CONFIG_SCHED_BORE */
 		if (se->vlag > 0) {
 			cfs_rq->nr_queued--;
 			if (se != cfs_rq->curr)
 				__dequeue_entity(cfs_rq, se);
 			se->vlag = 0;
-			place_entity(cfs_rq, se, flags);
+			place_entity(cfs_rq, se, 0);
 			if (se != cfs_rq->curr)
 				__enqueue_entity(cfs_rq, se);
 			cfs_rq->nr_queued++;
@@ -7000,7 +6987,7 @@ enqueue_task_fair(struct rq *rq, struct
 		util_est_enqueue(&rq->cfs, p);
 
 	if (flags & ENQUEUE_DELAYED) {
-		requeue_delayed_entity(se, flags);
+		requeue_delayed_entity(se);
 		return;
 	}
 
@@ -7018,7 +7005,7 @@ enqueue_task_fair(struct rq *rq, struct
 	for_each_sched_entity(se) {
 		if (se->on_rq) {
 			if (se->sched_delayed)
-				requeue_delayed_entity(se, flags);
+				requeue_delayed_entity(se);
 			break;
 		}
 		cfs_rq = cfs_rq_of(se);
-- 
2.40.2


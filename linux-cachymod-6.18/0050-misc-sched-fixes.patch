
Upstream miscellaneous scheduler fixes/cleanups:
https://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git/tag/?h=sched-urgent-2025-12-06

  - Remove a preempt-disable section in rt_mutex_setprio()
  - Fix hrtick() vs. scheduling context bug

Signed-off-by: Ingo Molnar <mingo@xxxxxxxxxx>

sched/rt: Remove a preempt-disable section in rt_mutex_setprio()
https://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git/commit/?h=sched/urgent&id=22abd832776b1317ae4c3f8a097c8b71bf83fb38

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 1711e9e501003a..ee7dfbf01792e5 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -7443,14 +7443,11 @@ void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)
 
 	check_class_changed(rq, p, prev_class, oldprio);
 out_unlock:
-	/* Avoid rq from going away on us: */
-	preempt_disable();
+	/* Caller holds task_struct::pi_lock, IRQs are still disabled */
 
 	rq_unpin_lock(rq, &rf);
 	__balance_callbacks(rq);
 	raw_spin_rq_unlock(rq);
-
-	preempt_enable();
 }
 #endif /* CONFIG_RT_MUTEXES */
 
-- 
2.47.1

sched/hrtick: Fix hrtick() vs. scheduling context
https://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git/commit/?h=sched/urgent&id=e38e5299747b23015b00b0109891815db44a2f30

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index fc358c1b6ca987..1711e9e501003a 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -872,7 +872,7 @@ static enum hrtimer_restart hrtick(struct hrtimer *timer)
 
 	rq_lock(rq, &rf);
 	update_rq_clock(rq);
-	rq->donor->sched_class->task_tick(rq, rq->curr, 1);
+	rq->donor->sched_class->task_tick(rq, rq->donor, 1);
 	rq_unlock(rq, &rf);
 
 	return HRTIMER_NORESTART;
-- 
2.47.1

sched:pelt: fix pelt clock sync when entering idle
https://lore.kernel.org/all/20260121163317.505635-1-vincent.guittot@linaro.org/
https://lore.kernel.org/all/aWpjrg0Z6QTiqnEE@vingu-cube/

From: Vincent Guittot
  We could have old clock_pelt_idle and clock_idle that were used to
  decay the util_avg of cfs task before migrating them which would ends
  up with decaying too much util_avg

  Could you try the fix below?

From: Alex Hoh (賀振坤)
  This fix looks good to me, and in my local testing, 
  RT utilization dropped significantly from 72 to 20.

From: Samuel Wu
  Fix also looks good to me, with RT util and power both
  dropped significantly and within baseline levels.

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 108213e94158..bea71564d3da 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -8909,12 +8909,6 @@ pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf
 			goto again;
 	}

-	/*
-	 * rq is about to be idle, check if we need to update the
-	 * lost_idle_time of clock_pelt
-	 */
-	update_idle_rq_clock_pelt(rq);
-
 	return NULL;
 }

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 65eb8f8c1a5d..9df6654ef84f 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -464,6 +464,12 @@ static void set_next_task_idle(struct rq *rq, struct task_struct *next, bool fir
 	scx_update_idle(rq, true, true);
 	schedstat_inc(rq->sched_goidle);
 	next->se.exec_start = rq_clock_task(rq);
+
+	/*
+	 * rq is about to be idle, check if we need to update the
+	 * lost_idle_time of clock_pelt
+	 */
+	update_idle_rq_clock_pelt(rq);
 }

 struct task_struct *pick_task_idle(struct rq *rq)
--
2.43.0

